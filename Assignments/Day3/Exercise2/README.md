## Exercise 2
This program implements the midpoint algorithm for computing an approximation of the mathematical constant pi. The code can be compiled by running `script.sh`, which will also take care of running all the experiments. In particular, the different processes are made to compute a local sum on a number of rectangles which is equal to N/num_procs. Subsequently, these local sums are aggregated through a reduction procedure invoked by the call to MPI_Reduce(...) having the last process as the receiving one (this is a kind of communication pattern known as n:1). Finally, this process sends the output to number 0, which will then print it, an operation which involves a synchronous blocking through MPI_Send(...)/MPI_Recv(...). In the meanwhile, the time taken to perform the approximation and reduce the result is measured. The results are summarized in the following plot, which also includes the OpenMP data from the previous lecture:

![alt text](https://github.com/pigozzif/DSSC/blob/master/Assignments/Day3/Exercise2/results_plot.png)

As we can see, the two parallel programming paradigms show quite a different behavior. In particular, multi-threading seems to dominate over a smaller number of threads/processes. Nevertheless, as the number increases, multi-threading stops scaling and actually worsens perfomance, as a result of the amount of synchronization required to manage a team of threads (in particular for the protection against the race condition). On the other hand, MPI keeps on scaling and slightly overperforms multi-threading at 40 processes, so a reasonable forecast would see it surclassing OpenMP even further (even if, for sure, scaling is not linear due to Amdahl's law). In fact, this problem seems to be more appropriate to a coarse-grained parallelism, since everything that has to be done is simply to compute a 'distributed' approximation of a constant and then aggregate the result once and for all, with very little communication in between.
