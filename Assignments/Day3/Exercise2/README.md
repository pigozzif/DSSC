## Exercise 2
This program implements the midpoint algorithm for computing an approximation of the mathematical constant pi. The code can be compiled by running `script.sh`, which will also take care of running all the experiments. In particular, the different processes are made to compute a local sum on a number of rectangles which is equal to N/num_procs (given N the total number of rectangles, here one billion). Subsequently, these local sums are aggregated through a reduction procedure invoked by the call to `MPI_Reduce(...)` having the last process as the receiving one (this is a kind of communication pattern known as n:1). Finally, this process sends the output to process 0, which will then print it, an operation involving a synchronous blocking through `MPI_Send(...)`/`MPI_Recv(...)`. In the meanwhile, the time taken to perform the approximation and reduce the result is measured. The results are summarized in the following plot, which also includes the OpenMP data from the previous lecture:

![alt text](https://github.com/pigozzif/DSSC/blob/master/Assignments/Day3/Exercise2/results_plot.png)

As we can see, the two parallel programming paradigms show a different behavior. In particular, multi-threading seems to dominate over a smaller number of threads/processes. Nevertheless, as the number of workers increases, multi-threading stops scaling and actually worsens performance, probably as a result of the amount of synchronization required to manage a team of threads (in particular for the protection against the race condition). On the other hand, MPI keeps on scaling, so a reasonable forecast would see it surclassing OpenMP even further (even if, for sure, scaling is not linear due to Amdahl's law). In fact, the reason MPI underperforms its competitor at the onset might be related to the overhead associated with the creation of a world of processes, which does not really scale as a function of their number.
