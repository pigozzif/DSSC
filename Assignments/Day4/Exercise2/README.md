## Exercise 2 - part 1

To accomplish the first part, you need to run `script_blocking.sh`. If the matrix size is smaller than 10, you will see the result printed on the stdout, while for larger matrices a file `distributed_matrix1.dat` will be created. The number of processes can be changed from within the bash script.

The program `distributed_blocking.c` initializes a distributed identity matrix of dimension (N,N), by striping it over the outer index, being the row index for a C-ordered matrix (of course, the reason is better memory access pattern). In the case the size of the matrix is not a multiple of the number of processes, we need to take care of the remainder, and this is done increasing of one the local dimension for all the processes whose rank is smaller than the rest. As we have seen in lecture, this is a handful way to avoid load unbalancing. Subsequently, we need to print/write the matrix. In the absence of a parallel file system, the best and only feasible way to do so is to send and enqueue all the distributed portions on the root process, which will work on them sequentially. In this program, the task is accomplished by making use of the blocking `MPI_Send` and `MPI_Recv` routines. Root process works on the distributed portions by subsequently overwriting its local buffer and then processing it, since we assume we will no longer need its portion, otherwise an additional array would be needed. While printing is a trivial feat, writing on a binary file (for sizes N > 10) is made possible by a bunch of functions defined in `stdlib.h`. In both this exercise and the following, for ease of manipulation, matrices are implemented as 1-d arrays and an appropriate `INDEX` macro has been defined.

## Exercise 2 - part 2

To accomplish the second part, you need to run `script_nonblocking.sh`, provided you have made it executable with `chmod +x script_nonblocking.sh`. If the matrix size is smaller than 10, you will see it printed on the stdout, while for larger matrices a file `distributed_matrix2.dat` will be created. The number of processes can be changed from within the bash script.

The program `distributed_nonblocking.c` performs the same operations and produces the same output as its part 1 counterpart. On the other side, the task has been fullfilled by making use of non-blocking communication. 
  The very first step in writing the non-blocking program is realizing that if we want to perform task parallelism and carry on two tasks that both include the usage of memory (communication and I/O) we need at least two buffers of memory. As a result, an auxiliary array is allocated by process 0 before looping over the other processes. In fact, what happens is that 0: 1) Receives from a process; 2) In the meanwhile, works on the previous one; 3) Stores the received buffer into the one it has just elaborated by swapping pointers (a copy would be very inefficient). Subsequently, the receival by process 0 of the distributed portions have been implemented with `MPI_Irecv`, which is in fact non-blocking. Notice that the call to `MPI_Send` by the others could have been transformed into `MPI_Isend` as well, but the effect would have been negligible since that is the only thing that those processes do and is overlapped somewhere else (there is no computation to overlap). 
  This is because we can overlap communication and I/O by noticing that each different local portion of the matrix can be transmitted independently from the others, provided that it is made available once we have to work on it (then the call to `MPI_Wait` just before swapping the pointers). Finally, we have achieved one of the main goals of parallel programming, that is overlapping communication and computation; as such, we can decrease the infamous time 't0' which represents a sequential portion of code that cannot be reduced by scaling over multiple workers. Notice we need a request handler for the receive.
