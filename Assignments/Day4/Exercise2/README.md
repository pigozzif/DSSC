## Exercise 2 - part 1

To accomplish the first part, you need to run `script_blocking.sh`, provided you have made it executable with `chmod +x script.sh`. If the matrix size is smaller than 10, you will see it printed on the stdout, while for larger matrices a file `distributed_matrix1.dat` will be created. The number of processes can be changed from within the bash script.

The program `distributed_blocking.c` initializes a distributed matrix of dimension (N,N), by striping it over the outer index, being the row index for a C-ordered matrix (of course, the reason is better memory access pattern). In the case the size of the matrix is not a multiple of the number of processes, we need to take care of the remainder, and this is done increasing of one the local dimension for all the processes such that their rank is smaller than the rest. As we have seen in lecture, this is an easy way to avoid load unbalancing. Subsequently, we need to print/write the matrix. In the absence of a parallel file system, the best and only feasible way to do so is to send and enqueue all the distributed portions on the root process, which will process them sequentially. In this program, the task is accomplished by making use of the blocking `MPI_Send` and `MPI_Recv` routines. Notice that we need to forward also the number of rows for each sub-matrix, since we have taken care of the rest. While printing is a trivial feat, writing on a binary file is made possible by a bunch of functions defined in `stdlib.h`.

## Exercise 2 - part 2

To accomplish the first part, you need to run `script_nonblocking.sh`, provided you have made it executable with `chmod +x script_nonblocking.sh`. If the matrix size is smaller than 10, you will see it printed on the stdout, while for larger matrices a file `distributed_matrix2.dat` will be created. The number of processes can be changed from within the bash script.

The program `distributed_nonblocking.c` performs the same operations and produces the same output as its part 1 counterpart. On the other side, the task has been fulfilled by making use of non-blocking communication. Namely, the send of the distributed portions to the root process, as well as their receival by process 0, have been implemented with `MPI_Isend` and `MPI_Irecv` respectively (which are in fact non-blocking). This is because we can overlap communication and I/O by noticing that each different row can be transmitted independetly from the others, provided that it is made available once we have to work on it (then the call to `MPI_Wait` just before I/O). For what concerns the send, `MPI_Wait` can be placed at the very end, just before deallocation.
